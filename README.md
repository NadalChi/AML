## Installation
* Clone the repo to your local machine using https://github.com/NadalChi/AML.git


```python
!git clone https://github.com/NadalChi/AML.git
```

    Cloning into 'AML'...
    remote: Enumerating objects: 4900, done.[K
    remote: Counting objects: 100% (4900/4900), done.[K
    remote: Compressing objects: 100% (4785/4785), done.[K
    remote: Total 4900 (delta 63), reused 4879 (delta 49), pack-reused 0[K
    Receiving objects: 100% (4900/4900), 7.04 MiB | 2.59 MiB/s, done.
    Resolving deltas: 100% (63/63), done.
    Updating files: 100% (5036/5036), done.


- Install this package first.


```python
!pip install googledrivedownloader
```

    Requirement already satisfied: googledrivedownloader in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (0.4)


## Download model

* Run the following command. Or under folder AML/code, then run download_model.py


```python
%cd AML/code
%run -i 'download_model.py'
```

    /Users/huangchingchi/Desktop/AML/code/AML/code
     == Download model == 
    Downloading 1pC9OGFrsV4l_JCUT5Mn6eZgZXsDxFL8a into ../model/finetuned_token_cls_model... Done.
     == Done == 


## Load sample data
* Load sample data from data set.  
* You can choose any data you want between 1.txt to 5023.txt.  
* For example, if you want to load single data set 10.txt, then you can call function as `aml.load_smaple_data(10)`.  
* The implement result shows as below.


```python
import AML_readme as aml
label, text = aml.load_smaple_data(4801)
print(label, text)
label, text = aml.load_smaple_data(4802)
print(label, text)
```

    []
     40æ­²æ¥Šå§“å¥³å­å› ç¼ºéŒ¢èŠ±ç”¨ï¼Œä»Šå¹´6æœˆé–“å¸¶è‘—å®‰å§“å°å¦¹ã€é™³å§“å°å¼Ÿåˆ°æ¸…æ°´å€ä¸€å®¶æª³æ¦”æ”¤ï¼Œæ¥Šå¥³å…ˆæŒç©å…·æ§å–ä»¤åº—å“¡ã€Œä¸å‡†å‹•ã€ï¼Œä¸¦ç”±å®‰å¥³è² è²¬æœåˆ®åº—å…§è²¡ç‰©ã€é™³ç”·å‰‡è² è²¬æŠŠé¢¨ï¼Œäº‹å¾Œ3äººè¢«æ•ï¼Œé™³ç”·é›–è¾¯ç¨±ã€Œæ˜¯è¢«é€¼è¿«çŠ¯æ¡ˆã€ï¼Œä»è¢«ä¾å¼·ç›œç½ªèµ·è¨´ã€‚èµ·è¨´æŒ‡å‡ºï¼Œä»Šå¹´6æœˆ2æ—¥æ¸…æ™¨ï¼Œæ¥Šå§“å¥³å­å¸¶è‘—é™³å§“å°å¼Ÿï¼ˆ39æ­²ï¼‰ï¼Œå…ˆåœ¨å½°åŒ–ç¸£ä¸­æ­£è·¯é™„è¿‘å¼·ç›œä¸€éƒ¨å°è½è»Šï¼Œä¹‹å¾Œç”±é™³ç”·è² è²¬é§•è»Šã€è¼‰è‘—æ¥Šå¥³åˆ°å°ä¸­å¸‚è¥¿å±¯å€æ¥å®‰å§“å¥³å­ï¼ˆ26æ­²ï¼‰ï¼Œæ¥è‘—3äººå…±ä¹˜1è»Šï¼Œæ²¿é€”å°‹æ‰¾çŠ¯æ¡ˆç›®æ¨™ï¼Œè¡Œç¶“æ¸…æ°´å€ä¸€å®¶æª³æ¦”æ”¤å‰æ™‚ï¼Œä¾¿æ±ºå®šä¸‹æ‰‹ã€‚æ‰‹æŒç©å…·æ§çš„æ¥Šå¥³èˆ‡å®‰å¥³å…ˆä¸‹è»Šé€²å…¥æª³æ¦”æ”¤ï¼Œç”±æ¥Šå¥³æŒæ§æåš‡åº—å“¡ã€Œåˆ¥å‹•ã€ï¼Œå®‰å¥³å‰‡é€²å…¥æ‹¿èµ°æª³æ¦”æ”¤å…§ç¾é‡‘å…±8300å…ƒï¼Œç·Šæ¥è‘—ï¼Œæ¥Šå¥³é›¢å»å‰é‚„å–èµ°åº—å…§11åŒ…é¦™è¸ï¼Œè€Œæ•´å€‹éç¨‹ï¼Œé™³ç”·éƒ½è² è²¬åœ¨è»Šä¸ŠæŠŠé¢¨ã€æ¥æ‡‰ã€‚è­¦æ–¹äº‹å¾Œç²å ±ä¸¦é€®ç²3äººï¼Œæ¥Šå¥³å¦æ‰¿çŠ¯æ¡ˆï¼Œè€Œå®‰å¥³ã€é™³ç”·éƒ½è¾¯ç¨±ã€Œæ˜¯å—æ¥Šå¥³é€¼è¿«ã€ï¼Œä¸”é™³ç”·é‚„ä¾›ç¨±ã€Œäº‹å‰å› ç‚ºæœ‰åƒè—¥ï¼Œæ ¹æœ¬è¨˜ä¸å¾—éç¨‹ï¼Œåªè¨˜å¾—æœ‰é–‹è»Šã€éƒ½æ˜¯è½æ¥Šå¥³æŒ‡ç¤ºã€ã€‚å°ä¸­æª¢æ–¹èªç‚ºï¼Œæ¥Šå¥³é›–æ˜¯ä¸»å°ï¼Œä½†é™³ç”·ã€å®‰å¥³æ˜é¡¯æœ‰è‡ªä¸»æ„è­˜ï¼Œå»æœªè¶æ©Ÿé›¢é–‹ã€é‚„å–„ç›¡æŠŠé¢¨ä¹‹è²¬ï¼Œæ˜é¡¯æ˜¯å…±çŠ¯ï¼Œä¾å¼·ç›œç½ªèµ·è¨´3äººã€‚é•åä¸Šè¿°è¦å®šè€…ï¼Œä¸­æ™‚é›»å­å ±æœ‰æ¬Šåˆªé™¤ç•™è¨€ï¼Œæˆ–è€…ç›´æ¥å°é–å¸³è™Ÿï¼è«‹ä½¿ç”¨è€…åœ¨ç™¼è¨€å‰ï¼Œå‹™å¿…å…ˆé–±è®€ç•™è¨€æ¿è¦å‰‡ï¼Œè¬è¬é…åˆã€‚
    ['æ—ç¹¼è˜‡', 'å¾è©©å½¥']
     ç‚ºé”æœ€ä½³ç€è¦½æ•ˆæœï¼Œå»ºè­°ä½¿ç”¨ Chromeã€Firefox æˆ– Microsoft Edge çš„ç€è¦½å™¨ã€‚ã€”è¨˜è€…å³æ˜‡å„’ï¼åŸºéš†å ±å°ã€•ç”·å­æ—ç¹¼è˜‡èˆ‡å‰è·†æ‹³é“åœ‹æ‰‹å¾è©©å½¥åˆ©ç”¨è²·è³£æ°´ç”¢çš„åç¾©ï¼Œå°å¤–å¸é‡‘è¶…é19å„„ï¼Œæª¢è­¦åµè¾¦æ™‚ï¼Œå…©äººäº’ç›¸å¸è²¬ï¼Œæ—ç”·è‡ªç¨±åªæ˜¯æŠ•è³‡äººï¼Œæä¾›æ”¯ç¥¨ã€å¸³æˆ¶çµ¦å¾å¥³ä½¿ç”¨ã€‚æª¢å¯Ÿå®˜å»æŸ¥å‡ºé›™æ–¹æœ‰å¤šç­†é‡‘æµå¾€ä¾†ï¼Œæ¶‰æ¡ˆæƒ…ç¯€é‡å¤§ï¼Œä¾æ³•å°‡ä»–èµ·è¨´ã€‚åŸºéš†åœ°æ–¹æ³•é™¢è¿‘æ—¥åµçµï¼Œä¾é•åéŠ€è¡Œæ³•åˆ¤è™•æ—ç”·æœ‰æœŸå¾’åˆ‘8å¹´åŠã€‚æª¢è­¦èª¿æŸ¥ï¼Œé–‹è¨­è£œç¿’ç­çš„å¾å¥³æ–¼2014å¹´èµ·åˆ©ç”¨å¾äº‹æ°´ç”¢å°é¡åœ˜è³¼ç­‰æ–¹å¼ï¼Œå‘å­¸ç”Ÿå®¶é•·åŠå‹äººæ¨éŠ·æ°´ç”¢é€²å£æŠ•è³‡æ–¹å¼ï¼Œå¸å¼•å¤šäººåƒåŠ ï¼Œå…¶ä¸­åˆ†ç‚ºè‡¨æ™‚å–®èˆ‡å›ºå®šå–®ï¼Œä½¯ç¨±å¯ä»¥ç²å¾—å¹´æ¯æœ€é«˜å¯ç²å¾—180%ï¼Œè®“æŠ•è³‡äººè¶¨ä¹‹è‹¥é¶©ï¼Œæ§è‘—å¤§æ‰¹è³‡é‡‘æŠ•è³‡ï¼›å¾Œä¾†å› æŠ•è³‡é¡é€æ¼¸å¢åŠ ï¼Œå¾å¥³ä¾¿æ‰¾ä¾†å¯ä»¥é–‹ç«‹æ”¯ç¥¨çš„æ—ç”·ä¸€èµ·åƒèˆ‡ã€‚æ—ç”·å‰‡æä¾›å¸³æˆ¶åŠæ”¯ç¥¨ï¼Œè®“å¾å¥³é‹è½‰ä½¿ç”¨ï¼ŒæŠ•è³‡äººå‰‡å°‡æŠ•è³‡è³‡é‡‘åŒ¯å…¥æ—ç”·çš„æˆ¶é ­å…§ï¼Œç›´åˆ°å¾Œä¾†ç™¼ç¾ï¼Œå¸³æˆ¶å…§çš„é¤˜é¡å·²ä¸å¤ æ”¯ä»˜æ”¯ç¥¨è²»ç”¨ï¼Œå› è€Œè·³ç¥¨ï¼Œæ¡ˆä»¶ä¹Ÿå› æ­¤æ›å…‰ã€‚æ—ç”·åµè¨Šæ™‚å‘æ³•å®˜è¾¯ç¨±ï¼Œè‡ªå·±çš„å°å­©åœ¨å¾å¥³æ•™å­¸çš„è£œç¿’ç­è£œç¿’ï¼Œé›™æ–¹æ›¾æœ‰å€Ÿè²¸é—œä¿‚ï¼Œå¾Œä¾†å¾å¥³é‚€è‡ªå·±æŠ•è³‡ï¼Œæ‰æœƒå‡ºå€Ÿæ”¯ç¥¨åŠå¸³æˆ¶ï¼ŒåŒ¯é€²å¸³æˆ¶çš„éŒ¢èªªæ˜¯æŠ•è³‡å» å•†ï¼Œä¸¦ä¸çŸ¥é“ä»–å€‘æ˜¯èª°ï¼Œè‡ªå·±åªæ˜¯å€Ÿç¥¨çµ¦å¾å¥³ä½¿ç”¨çš„æŠ•è³‡è€…è€Œå·²ã€‚å¾å¥³å‰‡å‘æ³•å®˜è¡¨ç¤ºï¼Œç•¶æ™‚æ˜¯æ—ç”·èªªä»–æœ‰æœ‹å‹åœ¨åšæ°´ç”¢è²¨æ«ƒæŠ•è³‡ï¼Œèªªåˆ©æ½¤ä¸éŒ¯ï¼Œæ‰€ä»¥æ‰ä»‹ç´¹æœ‹å‹æŠ•è³‡ã€‚æª¢è­¦èª¿æŸ¥å¾å¥³å¸é‡‘æ™‚ä¹Ÿç™¼ç¾ï¼Œå¾å¥³æ‰€æ”¯ä»˜çµ¦æŠ•è³‡äººçš„æ”¯ç¥¨ï¼Œå¤šç‚ºéƒ½æ˜¯æ—ç”·é–‹ç«‹ï¼Œä¸”æŠ•è³‡æ¬¾é …å¤§éƒ¨åˆ†éƒ½æ˜¯åŒ¯å…¥æ—ç”·è¨­ç«‹è‘—å¸³æˆ¶å…§ï¼Œè‹¥æ—ç”·å–®ç´”åªæ˜¯æŠ•è³‡è€…ï¼Œä¸å¯èƒ½å†’é‚£éº¼å¤§çš„é¢¨éšªã€‚æ³•å®˜èªç‚ºï¼Œæ—ç”·è‡ªç¨±æ˜¯æŠ•è³‡å¾å¥³æ°´ç”¢è²¨æ«ƒç”Ÿæ„ï¼Œä½†äº‹å¾Œå»å°‡æˆ¶é ­è£¡é¢çš„éŒ¢è½‰åˆ°æ”¯ç¥¨å¸³æˆ¶å…§ï¼Œä¸å¯èƒ½ä¸çŸ¥é“è‡ªå·±æˆ¶é ­å…§çš„è³‡é‡‘å¦‚ä½•æ‡‰ç”¨ï¼Œä¸”é€™äº›å¹´ä¾†é–‹ç«‹æ”¯ç¥¨é‡‘é¡é«˜é”åå¤šå„„å…ƒï¼Œè‹¥ä¸çŸ¥å¾å¥³æ„åœ–ï¼Œæ€éº¼å¯èƒ½è²¿ç„¶é–‹å‡ºã€‚æ³•å®˜èªå®šï¼Œæ—ç”·è¡Œç‚ºå·²ç¶“è§¸çŠ¯éŠ€è¡Œæ³•ï¼Œä¸”é‡‘é¡è¶…éå„„å…ƒï¼Œä¾é•åéæ³•ç¶“ç‡Ÿæ”¶å—å­˜æ¬¾æ¥­å‹™ç½ªåˆ¤è™•ä»–æœ‰æœŸå¾’åˆ‘8å¹´åŠï¼›æœªæ‰£æ¡ˆçš„å…±åŒçŠ¯ç½ªæ‰€å¾—3å„„1740è¬1åƒå…ƒï¼Œé™¤æ‡‰ç™¼é‚„è¢«å®³äººç²å¾—è«‹æ±‚æå®³è³ å„Ÿä¹‹äººå¤–ï¼Œæ‡‰èˆ‡å¾è©©å½¥å…±åŒæ²’æ”¶ä¹‹ï¼Œä¸èƒ½æˆ–ä¸å®œåŸ·è¡Œæ²’æ”¶æ™‚ï¼Œè¿½å¾µå…¶åƒ¹é¡ã€‚å¾å¥³ä¸€ã€äºŒå¯©è¢«é‡åˆ¤12å¹´åŠï¼Œç¶“å†æ¬¡ä¸Šè¨´å¾Œï¼Œæ›´ä¸€å¯©æ–¼ä»Šå¹´5æœˆåˆ¤è™•å¥¹æœ‰æœŸå¾’åˆ‘9å¹´åŠï¼Œæ¸›å°‘3å¹´åˆ‘æœŸã€‚


## Process data
* As we have the sample data and the lables. We have to convert each word in sample data to token.  
* And give each word a lable. What we are doing is classification. If the word belongs to AML-related focal persons. We set the label to one else we set it to zero.  
* The implement result shows as below.


```python
tokens = aml.process_data(label, text)
print(tokens)
```

    [(tensor([ 101,  711, 6809, 3297,  881, 3846, 6222, 3126, 3362, 8024, 2456, 6379,
             886, 4500,  510, 2772, 4638, 3846, 6222, 1690,  526, 6381, 5442, 1426,
            1285, 1027, 8027, 1825, 7384, 2845, 2193,  527, 4511, 2094, 3360, 5326,
            5722,  680, 1184, 6647, 2891, 6887, 1744, 2797, 2528, 6408, 2503, 1164,
            4500,  743, 1297, 3717,  772, 4638, 1399,  721, 8024, 2190, 1912, 1429,
            7032, 6631, 6814,  783, 8024, 3466, 6356,  903, 1215, 3198, 8024,  697,
             782,  757, 4685, 1319, 6569, 8024, 3360, 4511, 5632, 4917, 1372, 3221,
            2832, 6598,  782, 8024, 2990,  897, 3118, 4873,  510, 2362, 2787, 5314,
            2528, 1957,  886, 4500, 3466, 2175, 2135, 1316, 3389, 1139, 1352, 3175,
            3300, 1914, 5011, 7032, 3837, 2518, 3341, 8024, 3868, 3428, 2658, 5688,
            7028, 1920, 8024,  898, 3791, 2199,  800, 6629, 6401, 1825, 7384, 1765,
            3175, 3791, 7368, 6818, 3189,  903, 5310, 8024,  898, 6824, 1353, 7213,
            6121, 3791, 1161, 1905, 3360, 4511, 3300, 3309, 2530, 1152, 2399, 1288,
            3466, 6356, 6444, 3389, 8024, 2458, 6392, 6133,  739, 4408, 4638, 2528,
            1957,  754, 2399, 6629, 1164, 4500,  794,  752, 3717,  772, 2207, 7583,
            1730, 6579, 5023, 3175, 2466, 8024, 1403, 2110, 4495, 2157, 7270, 1350,
            1351,  782, 2972, 7218, 3717,  772, 6822, 1366, 2832, 6598, 3175, 2466,
            8024, 1429, 2471, 1914,  782, 1346, 1217, 8024, 1071,  704, 1146,  711,
             707, 3198, 1296,  680, 1743, 2137, 1296, 8024,  879, 4917, 1377,  809,
            5815, 2533, 2399, 2622, 3297, 7770, 1377, 5815, 2533, 8024, 6375, 2832,
            6598,  782, 6633,  722, 5735, 7909, 8024, 2942, 4708, 1920, 2821, 6598,
            7032, 2832, 6598, 8039, 1400, 3341, 1728, 2832, 6598, 7583, 6852, 3933,
            1872, 1217, 8024, 2528, 1957,  912, 2823, 3341, 1377,  809, 2458, 4989,
            3118, 4873, 4638, 3360, 4511,  671, 6629, 1346,  680, 3360, 4511, 1156,
            2990,  897, 2362, 2787, 1350, 3118, 4873, 8024, 6375, 2528, 1957, 6817,
            6760,  886, 4500, 8024, 2832, 6598,  782, 1156, 2199, 2832, 6598, 6598,
            7032, 3726, 1057, 3360, 4511, 4638, 2787, 1928, 1079, 8024, 4684, 1168,
            1400, 3341, 1355, 4385, 8024, 2362, 2787, 1079, 4638,  865, 7583, 2347,
             679, 1916, 3118,  802, 3118, 4873, 6589, 4500, 8024, 1728, 5445, 6663,
            4873, 8024, 3428,  816,  738, 1728, 3634, 3284, 1045, 3360, 4511,  903,
            6380, 3198, 1403, 3791, 2135, 6796, 4917, 8024, 5632, 2346, 4638, 2207,
            2111, 1762, 2528, 1957, 3136, 2110, 4638, 6133,  739, 4408, 6133,  739,
            8024, 1352, 3175, 3295, 3300,  955, 6587, 1068, 5143, 8024, 1400, 3341,
            2528, 1957, 6913, 5632, 2346, 2832, 6598, 8024, 2798,  833, 1139,  955,
            3118, 4873, 1350, 2362, 2787, 8024, 3726, 6822, 2362, 2787, 4638, 7178,
            6432, 3221, 2832, 6598, 1322, 1555, 8024, 2400,  679, 4761, 6887,  800,
             812, 3221, 6443, 8024, 5632, 2346, 1372, 3221,  955, 4873, 5314, 2528,
            1957,  886, 4500, 4638, 2832, 6598, 5442, 5445, 2347, 2528, 1957, 1156,
            1403, 3791, 2135, 6134, 4850, 8024, 2496, 3198, 3221, 3360, 4511, 6432,
             800, 3300, 3301, 1351, 1762,  976, 3717,  772, 6573, 3385, 2832, 6598,
            8024, 6432, 1164, 3883,  679, 7231, 8024, 2792,  809, 2798,  792, 5305,
            3301, 1351, 2832, 6598,  102]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
            0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0,
            0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
            0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
            0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
            0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
            0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
            0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
            0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
            0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
            0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
            0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
            0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
            0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
            0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
            0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
            0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
            0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
            0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
            0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
            0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (tensor([ 101, 3466, 6356, 6444, 3389, 2528, 1957, 1429, 7032, 3198,  738, 1355,
            4385, 8024, 2528, 1957, 2792, 3118,  802, 5314, 2832, 6598,  782, 4638,
            3118, 4873, 8024, 1914,  711, 6963, 3221, 3360, 4511, 2458, 4989, 8024,
             684, 2832, 6598, 3621, 7555, 1920, 6956, 1146, 6963, 3221, 3726, 1057,
            3360, 4511, 6392, 4989, 4708, 2362, 2787, 1079, 8024, 5735, 3360, 4511,
            1296, 5283, 1372, 3221, 2832, 6598, 5442, 8024,  679, 1377, 5543, 1088,
            6929,  720, 1920, 4638, 7599, 7372, 3791, 2135, 6371,  711, 8024, 3360,
            4511, 5632, 4917, 3221, 2832, 6598, 2528, 1957, 3717,  772, 6573, 3385,
            4495, 2692, 8024,  852,  752, 1400, 1316, 2199, 2787, 1928, 7027, 7481,
            4638, 7178, 6760, 1168, 3118, 4873, 2362, 2787, 1079, 8024,  679, 1377,
            5543,  679, 4761, 6887, 5632, 2346, 2787, 1928, 1079, 4638, 6598, 7032,
            1963,  862, 2418, 4500, 8024,  684, 6821,  763, 2399, 3341, 2458, 4989,
            3118, 4873, 7032, 7583, 7770, 6809, 1282, 1914,  783, 1039, 8024, 5735,
             679, 4761, 2528, 1957, 2692, 1745, 8024, 2582,  720, 1377, 5543, 6588,
            4197, 2458, 1139, 3791, 2135, 6371, 2137, 8024, 3360, 4511, 6121,  711,
            2347, 5307, 6239, 4306, 7213, 6121, 3791, 8024,  684, 7032, 7583, 6631,
            6814,  783, 1039, 8024,  898, 6824, 1353, 7478, 3791, 5307, 5852, 3119,
            1358, 2100, 3621,  689, 1218, 5389, 1161, 1905,  800, 3300, 3309, 2530,
            1152, 2399, 1288, 8039, 3313, 2807, 3428, 4638, 1066, 1398, 4306, 5389,
            2792, 2533,  783,  674, 1283, 1039, 8024, 7370, 2418, 1355, 6820, 6158,
            2154,  782, 5815, 2533, 6435, 3724, 2938, 2154, 6608,  985,  722,  782,
            1912, 8024, 2418,  680, 2528, 6408, 2503, 1066, 1398, 3766, 3119,  722,
            8024,  679, 5543, 2772,  679, 2139, 2809, 6121, 3766, 3119, 3198, 8024,
            6841, 2519, 1071,  817, 7583, 2528, 1957,  671,  510,  753, 2144, 6158,
            7028, 1161, 2399, 1288, 8024, 5307, 1086, 3613,  677, 6401, 1400, 8024,
            3291,  671, 2144,  754,  791, 2399, 3299, 1161, 1905, 1961, 3300, 3309,
            2530, 1152, 2399, 1288, 8024, 1121, 2208, 2399, 1152, 3309,  102]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
            0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
            0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
            0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
            0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
            0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
            0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
            0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
            0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
            0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
            0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0,
            0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
            0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
            0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]


## Train model
* Create training set and validation set.  
* The model will saved under folder '~/model'.


```python
trainset = aml.load_processed_data(1, 4000)
validset = aml.load_processed_data(4000, 5024)
BATCH_SIZE = 4

trainloader = aml.DataLoader(trainset, batch_size=BATCH_SIZE, 
                         collate_fn=aml.create_mini_batch)
validloader = aml.DataLoader(validset, batch_size=BATCH_SIZE, 
                         collate_fn=aml.create_mini_batch)
EPOCHS = 30
aml.train(trainloader, validloader, EPOCHS)
```

## Load model
* The model we provide was trained by using 1.txt to 4000.txt from data set.  
* Load the model we downloaded at begin. The path was set equal to '../model/finetuned_token_cls_model'.  
* You can also load the path where your model at.  


```python
model = aml.load_model()
```

    Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
    - This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
    - This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
    Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-chinese and are newly initialized: ['classifier.weight', 'classifier.bias']
    You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.


## Inference
* If you are doing inference. First argument is the article you are going to inference. Second argument is the model you loaded.  
* The inference result shows as below.


```python
text = aml.load_smaple_data(4801)[1]
aml.inference(text, model)
```




    []




```python
text = aml.load_smaple_data(4802)[1]
aml.inference(text, model)
```




    ['æ—ç¹¼è˜‡', 'å¾è©©å½¥']


