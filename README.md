## Installation
* Clone the repo to your local machine using https://github.com/NadalChi/AML.git


```python
!git clone https://github.com/NadalChi/AML.git
```

    Cloning into 'AML'...
    remote: Enumerating objects: 4914, done.
    remote: Counting objects: 100% (4914/4914), done.
    remote: Compressing objects: 100% (4795/4795), done.
    remote: Total 4914 (delta 71), reused 4887 (delta 53), pack-reused 0
    Receiving objects: 100% (4914/4914), 7.04 MiB | 747.00 KiB/s, done.
    Resolving deltas: 100% (71/71), done.
    Updating files: 100% (5036/5036), done.


- Install this package first.


```python
!pip install googledrivedownloader
```

    Collecting googledrivedownloader
      Using cached googledrivedownloader-0.4-py2.py3-none-any.whl (3.9 kB)
    Installing collected packages: googledrivedownloader
    Successfully installed googledrivedownloader-0.4


## Download model

* Run the following command. Or under folder AML/code, then run download_model.py


```python
%cd AML/code
%run -i 'download_model.py'
```

    /Users/huangchingchi/Desktop/AML/code/AML/code
     == Download model == 
    Downloading 1pC9OGFrsV4l_JCUT5Mn6eZgZXsDxFL8a into ../model/finetuned_token_cls_model... Done.
     == Done == 


## Load sample data
* Load sample data from data set.  
* You can choose any data you want between 1.txt to 5023.txt.  
* For example, if you want to load single data set 4800.txt, then you can call function as `aml.load_smaple_data(4800)`.  
* The implement result shows as below.


```python
import AML_readme as aml
label, text = aml.load_smaple_data(4801)
print(label, text)
label, text = aml.load_smaple_data(4800)
print(label, text)
```

    []
     40æ­²æ¥Šå§“å¥³å­å› ç¼ºéŒ¢èŠ±ç”¨ï¼Œä»Šå¹´6æœˆé–“å¸¶è‘—å®‰å§“å°å¦¹ã€é™³å§“å°å¼Ÿåˆ°æ¸…æ°´å€ä¸€å®¶æª³æ¦”æ”¤ï¼Œæ¥Šå¥³å…ˆæŒç©å…·æ§å–ä»¤åº—å“¡ã€Œä¸å‡†å‹•ã€ï¼Œä¸¦ç”±å®‰å¥³è² è²¬æœåˆ®åº—å…§è²¡ç‰©ã€é™³ç”·å‰‡è² è²¬æŠŠé¢¨ï¼Œäº‹å¾Œ3äººè¢«æ•ï¼Œé™³ç”·é›–è¾¯ç¨±ã€Œæ˜¯è¢«é€¼è¿«çŠ¯æ¡ˆã€ï¼Œä»è¢«ä¾å¼·ç›œç½ªèµ·è¨´ã€‚èµ·è¨´æŒ‡å‡ºï¼Œä»Šå¹´6æœˆ2æ—¥æ¸…æ™¨ï¼Œæ¥Šå§“å¥³å­å¸¶è‘—é™³å§“å°å¼Ÿï¼ˆ39æ­²ï¼‰ï¼Œå…ˆåœ¨å½°åŒ–ç¸£ä¸­æ­£è·¯é™„è¿‘å¼·ç›œä¸€éƒ¨å°è½è»Šï¼Œä¹‹å¾Œç”±é™³ç”·è² è²¬é§•è»Šã€è¼‰è‘—æ¥Šå¥³åˆ°å°ä¸­å¸‚è¥¿å±¯å€æ¥å®‰å§“å¥³å­ï¼ˆ26æ­²ï¼‰ï¼Œæ¥è‘—3äººå…±ä¹˜1è»Šï¼Œæ²¿é€”å°‹æ‰¾çŠ¯æ¡ˆç›®æ¨™ï¼Œè¡Œç¶“æ¸…æ°´å€ä¸€å®¶æª³æ¦”æ”¤å‰æ™‚ï¼Œä¾¿æ±ºå®šä¸‹æ‰‹ã€‚æ‰‹æŒç©å…·æ§çš„æ¥Šå¥³èˆ‡å®‰å¥³å…ˆä¸‹è»Šé€²å…¥æª³æ¦”æ”¤ï¼Œç”±æ¥Šå¥³æŒæ§æåš‡åº—å“¡ã€Œåˆ¥å‹•ã€ï¼Œå®‰å¥³å‰‡é€²å…¥æ‹¿èµ°æª³æ¦”æ”¤å…§ç¾é‡‘å…±8300å…ƒï¼Œç·Šæ¥è‘—ï¼Œæ¥Šå¥³é›¢å»å‰é‚„å–èµ°åº—å…§11åŒ…é¦™è¸ï¼Œè€Œæ•´å€‹éç¨‹ï¼Œé™³ç”·éƒ½è² è²¬åœ¨è»Šä¸ŠæŠŠé¢¨ã€æ¥æ‡‰ã€‚è­¦æ–¹äº‹å¾Œç²å ±ä¸¦é€®ç²3äººï¼Œæ¥Šå¥³å¦æ‰¿çŠ¯æ¡ˆï¼Œè€Œå®‰å¥³ã€é™³ç”·éƒ½è¾¯ç¨±ã€Œæ˜¯å—æ¥Šå¥³é€¼è¿«ã€ï¼Œä¸”é™³ç”·é‚„ä¾›ç¨±ã€Œäº‹å‰å› ç‚ºæœ‰åƒè—¥ï¼Œæ ¹æœ¬è¨˜ä¸å¾—éç¨‹ï¼Œåªè¨˜å¾—æœ‰é–‹è»Šã€éƒ½æ˜¯è½æ¥Šå¥³æŒ‡ç¤ºã€ã€‚å°ä¸­æª¢æ–¹èªç‚ºï¼Œæ¥Šå¥³é›–æ˜¯ä¸»å°ï¼Œä½†é™³ç”·ã€å®‰å¥³æ˜é¡¯æœ‰è‡ªä¸»æ„è­˜ï¼Œå»æœªè¶æ©Ÿé›¢é–‹ã€é‚„å–„ç›¡æŠŠé¢¨ä¹‹è²¬ï¼Œæ˜é¡¯æ˜¯å…±çŠ¯ï¼Œä¾å¼·ç›œç½ªèµ·è¨´3äººã€‚é•åä¸Šè¿°è¦å®šè€…ï¼Œä¸­æ™‚é›»å­å ±æœ‰æ¬Šåˆªé™¤ç•™è¨€ï¼Œæˆ–è€…ç›´æ¥å°é–å¸³è™Ÿï¼è«‹ä½¿ç”¨è€…åœ¨ç™¼è¨€å‰ï¼Œå‹™å¿…å…ˆé–±è®€ç•™è¨€æ¿è¦å‰‡ï¼Œè¬è¬é…åˆã€‚
    ['é™³æšå®—']
     ï¼ˆä¸­å¤®ç¤¾è¨˜è€…åŠ‰ä¸–æ€¡å°åŒ—27æ—¥é›»ï¼‰ä¿å®‰è­¦å¯Ÿç¬¬äºŒç¸½éšŠåˆ‘äº‹è­¦å¯Ÿå¤§éšŠå°éšŠé•·é™³æšå®—è¢«æ§åˆ©ç”¨æŸ¥ç·ä»¿å†’å“æ¡ˆï¼Œè©é¨™çŸ¥åæ³•å•†è·¯æ˜“å¨ç™»é¦¬çˆ¾æ‚Œè€¶å…¬å¸ï¼ˆLVï¼‰ï¼ŒäºŒå¯©ä»Šå¤©ä¾è²ªæ±¡ç­‰ç½ªåˆ¤åˆ‘2å¹´ã€ç·©åˆ‘4å¹´ï¼Œé ˆç¹³å…¬åº«15è¬å…ƒã€‚å¯ä¸Šè¨´ã€‚æª¢æ–¹èµ·è¨´æŒ‡å‡ºï¼Œé™³æšå®—åœ¨æ°‘åœ‹104å¹´11æœˆã€12æœˆé–“æŸ¥ç²LVä»¿å†’å•†å“ï¼Œæ˜çŸ¥æŸ¥ç²ä»¿å†’å“å¾Œæ‰€éœ€çš„è²¨è»Šè¼‰é‹è‡³è´“ç‰©åº«ï¼Œé‹è²»å‡ç”±å…¬æ¬¾æ”¯ä»˜ï¼Œå»ä½¯ç¨±é‹è²»é ˆç”±åŸå‘Šæ”¯ä»˜ã€‚å—LVå…¬å¸å§”è¨—å”åŠ©èª¿æŸ¥åŠé‘‘è­˜å·¥ä½œçš„é‘‘å®šå…¬å¸è² è²¬äººï¼Œä»£ç‚ºå¢Šæ¬¾é‹è²»æ–°å°å¹£2è¬5000å…ƒã€‚é‘‘å®šå…¬å¸è² è²¬äººå†å‘LVå…¬å¸è«‹æ¬¾ç²å‡†ã€‚æª¢æ–¹ä¾è²ªæ±¡æ²»ç½ªæ¢ä¾‹åˆ©ç”¨è·å‹™è©å–è²¡ç‰©ç½ªå°‡é™³æšå®—èµ·è¨´ã€‚ä¸€å¯©æ³•é™¢è€ƒé‡ä»–å¦æ‰¿çŠ¯è¡Œï¼Œç„¡å‰ç§‘ã€æœ‰æ‚”æ„ï¼Œå·²ç¹³å›2è¬5000å…ƒï¼Œä¸”å…¨æ¡ˆçŠ¯ç½ªæ‰€å¾—ä¸åˆ°5è¬å…ƒï¼Œåˆ¤ä»–2å¹´å¾’åˆ‘ã€ç·©åˆ‘4å¹´ï¼Œé ˆç¹³å…¬åº«15è¬å…ƒã€‚äºŒå¯©å°ç£é«˜ç­‰æ³•é™¢ä»Šå¤©å®£åˆ¤ï¼Œä¾è²ªæ±¡æ²»ç½ªæ¢ä¾‹åˆ©ç”¨è·å‹™è©å–è²¡ç‰©ç½ªäºŒç½ªï¼Œåˆ¤åˆ‘2å¹´ã€ç·©åˆ‘4å¹´ï¼Œé ˆç¹³å…¬åº«15è¬å…ƒã€‚å¯ä¸Šè¨´ã€‚


## Process data
* As we have the sample data and the lables. We have to convert each word in sample data to token.  
* And give each word a lable. What we are doing is classification. If the word belongs to AML-related focal persons. We set the label to one else we set it to zero.  
* The implement result shows as below.


```python
tokens = aml.process_data(label, text)
print(tokens)
```

    [(tensor([ 101, 8020,  704, 1925, 4852, 6381, 5442, 1155,  686, 2592, 1378, 1266,
            3189, 4510, 8021,  924, 2128, 6356, 2175, 5018,  753, 2600, 7339, 1152,
             752, 6356, 2175, 1920, 7339, 2207, 7339, 7270, 7357, 2813, 2134, 6158,
            2971, 1164, 4500, 3389, 5351,  820, 1088, 1501, 3428, 8024, 6400, 7745,
            4761, 1399, 3791, 1555, 6662, 3211, 2014, 4633, 7716, 2209, 2635, 5456,
            1062, 1385, 8020, 8021, 8024,  753, 2144,  791, 1921,  898, 6576, 3738,
            5023, 5389, 1161, 1152, 2399,  510, 5353, 1152, 2399, 8024, 7557, 5373,
            1062, 2417,  674, 1039, 1377,  677, 6401, 3466, 3175, 6629, 6401, 2900,
            1139, 8024, 7357, 2813, 2134, 1762, 3696, 1744, 2399, 3299,  510, 3299,
            7313, 3389, 5815,  820, 1088, 1555, 1501, 8024, 3209, 4761, 3389, 5815,
             820, 1088, 1501, 1400, 2792, 7444, 4638, 6573, 6756, 6770, 6817, 5635,
            6597, 4289, 2417, 8024, 6817, 6589, 1772, 4507, 1062, 3621, 3118,  802,
            8024, 1316,  879, 4917, 6817, 6589, 7557, 4507, 1333, 1440, 3118,  802,
            1358, 1062, 1385, 1999, 2805, 1291, 1221, 6444, 3389, 1350, 7063, 6399,
            2339,  868, 4638, 7063, 2137, 1062, 1385, 6566, 6569,  782, 8024,  807,
             711, 1807, 3621, 6817, 6589, 3173, 1378, 2355,  674, 1039, 7063, 2137,
            1062, 1385, 6566, 6569,  782, 1086, 1403, 1062, 1385, 6435, 3621, 5815,
            1114, 3466, 3175,  898, 6576, 3738, 3780, 5389, 3340,  891, 1164, 4500,
            5466, 1218, 6400, 1357, 6568, 4289, 5389, 2199, 7357, 2813, 2134, 6629,
            6401,  671, 2144, 3791, 7368, 5440, 7030,  800, 1788, 2824, 4306, 6121,
            8024, 3187, 1184, 4906,  510, 3300, 2637, 2692, 8024, 2347, 5373, 1726,
             674, 1039, 8024,  684, 1059, 3428, 4306, 5389, 2792, 2533,  679, 1168,
             674, 1039, 8024, 1161,  800, 2399, 2530, 1152,  510, 5353, 1152, 2399,
            8024, 7557, 5373, 1062, 2417,  674, 1039,  753, 2144, 1378, 3968, 7770,
            5023, 3791, 7368,  791, 1921, 2146, 1161, 8024,  898, 6576, 3738, 3780,
            5389, 3340,  891, 1164, 4500, 5466, 1218, 6400, 1357, 6568, 4289, 5389,
             753, 5389, 8024, 1161, 1152, 2399,  510, 5353, 1152, 2399, 8024, 7557,
            5373, 1062, 2417,  674, 1039, 1377,  677, 6401,  102]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
            0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
            0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
            0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
            0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
            0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
            0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
            0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
            0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
            0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
            0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
            0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
            0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
            0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]


## Train model
* Create training set and validation set.  
* The model will saved under folder `'~/model'`.


```python
trainset = aml.load_processed_data(1, 4000)
validset = aml.load_processed_data(4000, 5024)
BATCH_SIZE = 4

trainloader = aml.DataLoader(trainset, batch_size=BATCH_SIZE, 
                         collate_fn=aml.create_mini_batch)
validloader = aml.DataLoader(validset, batch_size=BATCH_SIZE, 
                         collate_fn=aml.create_mini_batch)
EPOCHS = 30
aml.train(trainloader, validloader, EPOCHS)
```

## Load model
* The model we provide was trained by using 1.txt to 4000.txt from data set.  
* Load the model we downloaded at begin. The path was set equal to `'../model/finetuned_token_cls_model'`.  
* You can also load the path where your model at.  


```python
model = aml.load_model()
```

    Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
    - This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
    - This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
    Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-chinese and are newly initialized: ['classifier.weight', 'classifier.bias']
    You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.


## Inference
* If you are doing inference. First argument is the article you are going to inference. Second argument is the model you loaded.  
* The inference result shows as below.


```python
text = aml.load_smaple_data(4801)[1]
aml.inference(text, model)
```




    []




```python
text = aml.load_smaple_data(4802)[1]
aml.inference(text, model)
```




    ['å¾è©©å½¥', 'æ—ç¹¼è˜‡']


